{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bf3a10",
   "metadata": {},
   "source": [
    "# LOAD IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72b4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import load_img_cache\n",
    "path = \"./save/keyframes/frame_0013.webp\"\n",
    "image = load_img_cache(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b0852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 854, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769e916",
   "metadata": {},
   "source": [
    "# BEIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f045cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "beit_tokenizer_path = \"F:\\\\UNIVERSITY\\\\Contest\\\\AIC\\\\workspace\\\\myworkspace\\\\model\\\\beit3_semantic.spm\"\n",
    "beit_model_semantic_path = \"F:\\\\UNIVERSITY\\\\Contest\\\\AIC\\\\workspace\\\\myworkspace\\\\model\\\\beit3_large_patch16_224_nlvr2.pth\"\n",
    "beit_model_retrieval_path = \"F:\\\\UNIVERSITY\\\\Contest\\\\AIC\\\\workspace\\\\myworkspace\\\\model\\\\beit3_large_patch16_384_f30k_retrieval.pth\"\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(beit_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "597a131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.beit.unilm.beit3.modeling_finetune import beit3_large_patch16_384_retrieval, beit3_large_patch16_224_nlvr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint_semantic = torch.load(beit_model_semantic_path, map_location=device)\n",
    "checkpoint_retrieval = torch.load(beit_model_retrieval_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_semantic = beit3_large_patch16_224_nlvr2(pretrained=True)\n",
    "model_semantic.load_state_dict(checkpoint_semantic['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5516170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_retrieval = beit3_large_patch16_384_retrieval(pretrained=True)\n",
    "model_retrieval.load_state_dict(checkpoint_retrieval['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3becc",
   "metadata": {},
   "source": [
    "# SPLIT FFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a1f4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils.registry import registry\n",
    "\n",
    "class FrameSplitter:\n",
    "    def __init__(self, interval: int):\n",
    "        self.writer = registry.get_writer(\"common\")\n",
    "        self.interval = interval\n",
    "\n",
    "    def split_frames(\n",
    "            self,\n",
    "            source  : str,\n",
    "            save_dir: str = None, \n",
    "            is_saved: bool = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "            Spliting video into frame\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            - source: mp4 video path\n",
    "            - save_dir: directory where frames is saved\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        if is_saved:\n",
    "            if save_dir==None:\n",
    "                print(\"Please provide valid save dir\")\n",
    "                raise ValueError\n",
    "                if not os.path.exists(save_dir):\n",
    "                    print(\"Create save directory\")\n",
    "                    os.mkdir(save_dir)\n",
    "\n",
    "        #-- Setup config\n",
    "        interval_sec = 2\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) # frame per second\n",
    "        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        num_skip_frames = interval_sec * fps\n",
    "\n",
    "        #-- Split frame\n",
    "        frames = []\n",
    "        frame_id = 0\n",
    "        saved_count = 0\n",
    "        if cap.isOpened() == False:\n",
    "            print('Cap is not open')\n",
    "\n",
    "        # print(f\"Start splitting - Total frames: {total_frames}\")\n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = self.cap_frame(cap, frame_id=frame_id)\n",
    "            # print(f\"Frame {frame_id} - Type: {type(frame)}\")\n",
    "            #~ Save frame\n",
    "            if is_saved and frame is not None:\n",
    "                save_path = os.path.join(save_dir, f'frame_{saved_count:04d}.webp')\n",
    "                self.save_frame(save_path, frame)\n",
    "\n",
    "            #~ Yield each frame\n",
    "            if frame is not None: frames.append(frame)\n",
    "            if not ret:\n",
    "                #~~ Save last frame\n",
    "                if frame_id - num_skip_frames < total_frames:\n",
    "                    ret, frame = self.cap_frame(cap, frame_id=total_frames - 1)\n",
    "                    if is_saved and frame is not None:\n",
    "                        save_path = os.path.join(save_dir, f'frame_{saved_count:04d}.webp')\n",
    "                        self.save_frame(save_path, frame)\n",
    "                    saved_count += 1\n",
    "                                \n",
    "                break\n",
    "            frame_id += num_skip_frames\n",
    "            saved_count += 1\n",
    "\n",
    "        print(f\"Splitting {saved_count} frames from video\")\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def cap_frame(self, cap, frame_id):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        ret, frame = cap.read()\n",
    "        return ret, frame\n",
    "\n",
    "    def save_frame(self, save_path, frame):\n",
    "        cv2.imwrite(save_path, frame, [int(cv2.IMWRITE_WEBP_QUALITY), 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec9b5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FrameSplitter(interval=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6e5001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 36 frames from video\n"
     ]
    }
   ],
   "source": [
    "source = \"data/video.mp4\"\n",
    "frames = splitter.split_frames(source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e9a84",
   "metadata": {},
   "source": [
    "# EXTRACT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adf4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba05c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_from_path(image_path: str, image_size=224):\n",
    "    \"\"\"Transform a single image.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    try:\n",
    "        with load_img_cache(image_path).convert('RGB') as img:\n",
    "            return transform(img).unsqueeze(0).to(self.device)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_image(img: np.ndarray, image_size=224):\n",
    "    \"\"\"Transform a single image.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    if img.shape[0] > 3:\n",
    "        img = img.transpose(2, 0, 1)\n",
    "    try:\n",
    "        return transform(torch.tensor(img)).to(\"cpu\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image: {e}\")\n",
    "        return None\n",
    "    # return transform(torch.tensor(img)).unsqueeze(0).to(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cde7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = frames[10]\n",
    "process_images = process_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580d549",
   "metadata": {},
   "source": [
    "## ENCODE IMAGE USING BEIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4e0b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.registry import registry\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from torchvision import transforms\n",
    "from utils.beit.unilm.beit3.modeling_finetune import beit3_base_patch16_224_retrieval, beit3_large_patch16_224_nlvr2\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from utils.utils import load_img_cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BEiTImangeEncoder:\n",
    "    def __init__(self, feat_type):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.config = registry.get_config(\"beit\")\n",
    "        self.writer = registry.get_writer(\"common\")\n",
    "        self.feat_type = feat_type\n",
    "\n",
    "        # self.build_task()\n",
    "\n",
    "    #-- BUILD\n",
    "    def build_task(self):\n",
    "        beit_type_config = self.config.get(self.feat_type, None)\n",
    "        if beit_type_config==None:\n",
    "            print(f\"Feature type {self.feat_type} unavailable\")\n",
    "            assert ValueError\n",
    "\n",
    "        beit_model_path = beit_type_config[\"model_path\"]\n",
    "        beit_tokenizer_path = beit_type_config[\"tokenizer_path\"]\n",
    "        if self.feat_type==\"retrieval\":\n",
    "            self.model = beit3_large_patch16_224_nlvr2(pretrained=True)\n",
    "        elif self.feat_type==\"semantic\":\n",
    "            self.model = beit3_large_patch16_224_nlvr2(pretrained=True)\n",
    "        \n",
    "        checkpoint = torch.load(beit_model_path, map_location=self.device)\n",
    "        self.tokenizer = XLMRobertaTokenizer(beit_tokenizer_path)\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    #-- FUNCTION\n",
    "    def process_image_from_path(self, image_path: str):\n",
    "        \"\"\"Transform a single image.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        try:\n",
    "            with load_img_cache(image_path).convert('RGB') as img:\n",
    "                return transform(img).unsqueeze(0).to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_image(self, img: np.ndarray, image_size=224):\n",
    "        \"\"\"Transform a single image.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        if img.shape[0] > 3:\n",
    "            img = img.transpose(2, 0, 1)\n",
    "        try:\n",
    "            return transform(torch.tensor(img)).to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process image: {e}\")\n",
    "            return None\n",
    "        \n",
    "    #-- Encode frame\n",
    "    def encode_frames(self, model, frames, batch_size=4):\n",
    "        \"\"\"\n",
    "            Function:\n",
    "            ---------\n",
    "                Encode all frames in one single video shot\n",
    "\n",
    "            Params:\n",
    "            ------\n",
    "                frames: List[np.ndarray] - W, H, C\n",
    "                    - Frame from frame splitting modules\n",
    "        \"\"\"\n",
    "        encoding_list = []\n",
    "        with torch.no_grad():\n",
    "            for start_idx in tqdm(range(0, len(frames), batch_size), desc=\"Processing and encoding images\"):\n",
    "                batch_frames = frames[start_idx:start_idx + batch_size]\n",
    "                batch_tensors = []\n",
    "\n",
    "                # Preprocess images in the batch\n",
    "                for frame_id, frame in enumerate(frames):\n",
    "                    try:\n",
    "                        image_tensor = self.process_image(frame)\n",
    "                        if image_tensor is not None:\n",
    "                            batch_tensors.append(image_tensor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to process frame {start_idx + frame_id}: {e}\")\n",
    "\n",
    "                if not batch_tensors:\n",
    "                    print(f\"No valid images in batch {start_idx}-{start_idx + batch_size}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Stack tensors and move to device\n",
    "                batch_images = torch.stack(batch_tensors).to(self.device)\n",
    "\n",
    "                # Encode images\n",
    "                try:\n",
    "                    image_features, _ = model(image=batch_images, only_infer=True)\n",
    "                    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                    encoding_list.extend(image_features.cpu().numpy().astype(np.float32))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during encoding batch {start_idx}-{start_idx + batch_size}: {e}\")\n",
    "\n",
    "        return encoding_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35aa388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing and encoding images:   0%|          | 0/9 [00:28<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "beit_encoder = BEiTImangeEncoder(\"retrieval\")\n",
    "embed = beit_encoder.encode_frames(model=model_retrieval, frames=frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
