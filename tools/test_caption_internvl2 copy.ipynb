{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714ea19a",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a45c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from icecream import  ic\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/data2/npl/ViInfographicCaps/contest/AIC/VideoFrameRetrieval\")\n",
    "from utils.transform import Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e808d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_image_path(root):\n",
    "    all_frames = glob.glob(os.path.join(root, \"**\", \"*.jpg\"), recursive=True)\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ce7bf",
   "metadata": {},
   "source": [
    "## LOAD IMAGE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940582f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dir = \"/data2/npl/ViInfographicCaps/contest/AIC/data/frames/keyframes\"\n",
    "list_frame_of_video_dir = [os.path.join(frames_dir, video_id) for video_id in os.listdir(frames_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78153a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir = list_frame_of_video_dir[0]\n",
    "all_test_image_paths = get_all_image_path(frames_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e783d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161695"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa3ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    Image.open(image_path)\n",
    "    for image_path in all_test_image_paths[:4]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d013c",
   "metadata": {},
   "source": [
    "## LOAD MODEL AND RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bb58c",
   "metadata": {},
   "source": [
    "# TEST CODE VERSION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef44e9ff96d44d1b01498358e1307a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "# If you set `load_in_8bit=True`, you will need two 80GB GPUs.\n",
    "# If you set `load_in_8bit=False`, you will need at least three 80GB GPUs.\n",
    "path = '/data2/npl/ViInfographicCaps/model/InternVL3-8B'\n",
    "# device_map = split_model(path)\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    ").to(\"cuda:3\").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac277c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c104a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values_1 = load_image(all_test_image_paths[0], max_num=12).to(torch.bfloat16).to(\"cuda:3\")\n",
    "pixel_values_2 = load_image(all_test_image_paths[1], max_num=12).to(torch.bfloat16).to(\"cuda:3\")\n",
    "pixel_values = torch.cat([pixel_values_1, pixel_values_2], dim=0)\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "num_patches_list = [pixel_values_1.size(0), pixel_values_2.size(0)]\n",
    "\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                num_patches_list=num_patches_list,\n",
    "#                                history=None, return_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "# num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "# question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                num_patches_list=num_patches_list,\n",
    "#                                history=None, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_template = \"Image-{id}: <image>\\n\"\n",
    "prefix = \"\".join([prefix_template.format(id=i+1) for i in range(2)])\n",
    "# question = '<image>\\nPlease describe the image shortly.'\n",
    "prompt = prefix + \"\"\"\n",
    "    Carefully analyze the image below and describe everything you can see in the image in as much detail as possible. \n",
    "    Describe all visible objects and people and their actions, positions, and relationships.\n",
    "    Mention colors, location, and time of day of the scene in the images.\n",
    "\n",
    "    ### Captions return in the following format:\n",
    "        Image 1: <Caption 1>\n",
    "        ...\n",
    "        Image n: <Caption n>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b75c90e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = model.chat(tokenizer, pixel_values, prompt, generation_config, num_patches_list=num_patches_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78141d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Image-1: The image shows shows an indoor scene where a person in a green shirt is holding a clear plastic bag filled with various items, likely gifts or small boxes, while another person in the background holds a yellow object. The setting appears to be a store or a gift shop, as indicated by the presence of a Christmas tree with decorations and shelves stocked with items in the background. The blurred faces suggest movement or a low-resolution capture. The watermark \"tuoi tre TV\" is visible in the top right corner.\\n\\nImage-2: In the image, a person wearing a bright green shirt and blue jeans is holding a box labeled \"HANOMO\" in a public area, possibly a street or transit area, with a white bus visible in the background. The individual appears to be interacting with another person who is partially visible and wearing a black top. The background includes a shop with a yellow sign that reads \"HANOMO\" and various items such as water bottles displayed outside. The setting suggests an urban environment with trees lining the street and a mix of wet and dry ground. The image has a watermark with \"tuoi tre TV\" in the top right corner.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a0f834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "parts = re.split(r'Image-\\d+\\s*', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "719fa5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "parts.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efc02f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ': The image shows shows an indoor scene where a person in a green shirt is holding a clear plastic bag filled with various items, likely gifts or small boxes, while another person in the background holds a yellow object. The setting appears to be a store or a gift shop, as indicated by the presence of a Christmas tree with decorations and shelves stocked with items in the background. The blurred faces suggest movement or a low-resolution capture. The watermark \"tuoi tre TV\" is visible in the top right corner.\\n\\n',\n",
       " ': In the image, a person wearing a bright green shirt and blue jeans is holding a box labeled \"HANOMO\" in a public area, possibly a street or transit area, with a white bus visible in the background. The individual appears to be interacting with another person who is partially visible and wearing a black top. The background includes a shop with a yellow sign that reads \"HANOMO\" and various items such as water bottles displayed outside. The setting suggests an urban environment with trees lining the street and a mix of wet and dry ground. The image has a watermark with \"tuoi tre TV\" in the top right corner.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
