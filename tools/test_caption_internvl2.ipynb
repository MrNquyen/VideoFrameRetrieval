{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714ea19a",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from icecream import  ic\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/data2/npl/ViInfographicCaps/contest/AIC/VideoFrameRetrieval\")\n",
    "from utils.transform import Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e808d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_image_path(root):\n",
    "    all_frames = glob.glob(os.path.join(root, \"**\", \"*.jpg\"), recursive=True)\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ce7bf",
   "metadata": {},
   "source": [
    "## LOAD IMAGE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940582f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dir = \"/data2/npl/ViInfographicCaps/contest/AIC/data/frames/keyframes\"\n",
    "list_frame_of_video_dir = [os.path.join(frames_dir, video_id) for video_id in os.listdir(frames_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78153a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = list_frame_of_video_dir[0]\n",
    "all_test_image_paths = get_all_image_path(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa3ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    Image.open(image_path)\n",
    "    for image_path in all_test_image_paths[:4]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d013c",
   "metadata": {},
   "source": [
    "## LOAD MODEL AND RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0096907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    ### Instruction:\n",
    "        - You are an expert language processor. Follow the instructions carefully.\n",
    "        - Only return the answer in the exact format specified. Do not explain or add anything extra.\n",
    "\n",
    "    ### Task:\n",
    "        - Carefully analyze the image below and describe everything you can see in the image in as much detail as possible.\n",
    "        - Categories you should include in the caption are:\n",
    "            + All Visible objects and people.\n",
    "            + Describe their actions, positions, and relationships.\n",
    "            + Mention colors, location, and time of day.\n",
    "            + Do not overlook small or subtle elements, such as facial expressions, hand gestures, background objects, shadows, or reflections\n",
    "\"\"\"\n",
    "prompt = '<image>\\nPlease describe the image shortly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "303407a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| model_path: '/data2/npl/ViInfographicCaps/model/InternVL2_5-1B'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/npl/luannt/envs/taru/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.registry import registry\n",
    "from icecream import  ic\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from utils.transform import Transform\n",
    "\n",
    "class ImageCaptioner:\n",
    "    def __init__(self):\n",
    "        self.config = config = {\n",
    "            \"model_path\": \"/data2/npl/ViInfographicCaps/model/InternVL2_5-1B\"\n",
    "        }\n",
    "        self.device = \"cuda:3\"\n",
    "        self.transform = Transform(image_size=448)\n",
    "        #~ Load model\n",
    "        self.load_model()\n",
    "\n",
    "    def convert_image_type(self, image):\n",
    "        if type(image) == np.ndarray:\n",
    "            image = Image.fromarray(image)\n",
    "        elif type(image) == torch.Tensor:\n",
    "            to_pil = transforms.ToPILImage()\n",
    "            image = to_pil(image)\n",
    "        return image\n",
    "\n",
    "    def load_model(self):\n",
    "        model_path = self.config[\"model_path\"]\n",
    "        ic(model_path)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_flash_attn=True,\n",
    "            trust_remote_code=True\n",
    "        ).eval().to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):\n",
    "        best_ratio_diff = float('inf')\n",
    "        best_ratio = (1, 1)\n",
    "        area = width * height\n",
    "        for ratio in target_ratios:\n",
    "            target_aspect_ratio = ratio[0] / ratio[1]\n",
    "            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "            if ratio_diff < best_ratio_diff:\n",
    "                best_ratio_diff = ratio_diff\n",
    "                best_ratio = ratio\n",
    "            elif ratio_diff == best_ratio_diff:\n",
    "                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                    best_ratio = ratio\n",
    "        return best_ratio\n",
    "\n",
    "    def dynamic_preprocess(self, image, min_num=1, max_num=12, image_size=224, use_thumbnail=False):\n",
    "        orig_width, orig_height = image.size\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "\n",
    "        # calculate the existing image aspect ratio\n",
    "        target_ratios = set(\n",
    "            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "            i * j <= max_num and i * j >= min_num)\n",
    "        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "        # find the closest aspect ratio to the target\n",
    "        target_aspect_ratio = self.find_closest_aspect_ratio(\n",
    "            aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "        )\n",
    "\n",
    "        # calculate the target width and height\n",
    "        target_width = image_size * target_aspect_ratio[0]\n",
    "        target_height = image_size * target_aspect_ratio[1]\n",
    "        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "        # resize the image\n",
    "        resized_img = image.resize((target_width, target_height))\n",
    "        processed_images = []\n",
    "        for i in range(blocks):\n",
    "            box = (\n",
    "                (i % (target_width // image_size)) * image_size,\n",
    "                (i // (target_width // image_size)) * image_size,\n",
    "                ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                ((i // (target_width // image_size)) + 1) * image_size\n",
    "            )\n",
    "            # split the image\n",
    "            split_img = resized_img.crop(box)\n",
    "            processed_images.append(split_img)\n",
    "        assert len(processed_images) == blocks\n",
    "        if use_thumbnail and len(processed_images) != 1:\n",
    "            thumbnail_img = image.resize((image_size, image_size))\n",
    "            processed_images.append(thumbnail_img)\n",
    "        return processed_images\n",
    "    \n",
    "    def tokenize_image(self, image, max_num=12):\n",
    "        image = self.convert_image_type(image).convert(\"RGB\")\n",
    "        images = self.dynamic_preprocess(\n",
    "            image=image, \n",
    "            use_thumbnail=True, \n",
    "            max_num=max_num\n",
    "        )\n",
    "        transform = self.transform.transform_from_PIL()\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return pixel_values\n",
    "    \n",
    "    def caption(self, image):\n",
    "        pixel_values = self.tokenize_image(image, max_num=12).to(torch.float16).to(self.device)\n",
    "        generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "        question = '<image>\\nPlease describe the image shortly.'\n",
    "        response = self.model.chat(self.tokenizer, pixel_values, question, generation_config)\n",
    "        return response\n",
    "\n",
    "\n",
    "captioner = ImageCaptioner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca921a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image shows a red frying pan on a stove with a blue flame underneath. There is some oil or butter in the pan, and a small amount of garlic is being saut√©ed.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captioner.caption(images[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
